# 自己写的

## 使用流程

Classifier是分类器的库文件，现在写了KNN、SVM、Tree（决策树）三个类，决策树类处理的不是很好。

### 数据处理

`load_file(file_path)`

用于读取文件中的数据，并对非数字特征进行数字编码，返回处理之后的特征数据 `data_list `和标签列表 `label_list`

- 特别的，决策树它的数据处理不一样，因为是按照《机器学习实战》书上的代码设计的，因此它返回的是全部数据 `data_list` （ 包括特征数据及对应的分类标签）和特征名称 `feature_name`

```python
data_list, label_list = Classifier.load_file(file_path)

# Tree
data_list, feature_name = Classifier.load_file('./data/drug.txt', 'Tree')
```

`Classifier.standard_data(data_list)`

对数据进行标准化处理，处理结果是[-1, 1]范围内的数

```python
data_list = Classifier.standard_data(data_list)
```

### 创建分类器

传参一般是特征矩阵和标签列表还有超参数。

- 特别的，决策树的`feature_list`是分类类别的名称，因此它的数据处理也不一样。这是因为是按照书上的示例代码设计的，还没有完全调整好。

```python
# 创建KNN分类器对象，一部分数据集用作训练，一部分用作测试
knn = Classifier.KNN(data_list,label_list, 3)

# 创建SVM分类器对象
svm1 = Classifier.SVM(data_list,label_list, c=100, toler=1e-4, max_iter=90,k_tup=('rbf', 1))

# 创建决策树分类器对象
data_list, feature_list = Classifier.load_file('./data/drug.txt', 'Tree') 
tree = Classifier.Tree(data_list, feature_list)
```

### 优化超参数

采用的是网格搜索法，需要提前设置超参数的范围，以字典的形式传入。

**设置超参数范围**

```python
# KNN分类器的超参数
params = {
    'k': range(3, 5)
}

# SVM的超参数
params = {
    'model': 'ovr',  				# 多分类器采用的策略，默认是ovr
    'kernel': ['rbf', 'line'],  	# 核函数类型，默认是rbf
    'c': [1, 10, 100],  			# 目标函数的惩罚系数，默认是1
    'gamma': [1e-2, 1e-1, 1]  		# 核函数的系数，默认是1.3
}
```

**进行网格搜索**

`grid_search()`函数需要传入参数字典，可以设置随机数种子，返回值是最佳参数元组和最佳验证错误率

```python
# 进行网格搜索

# KNN
knn.grid_search(params, seed)

# SVM
svm1.grid_search(params, seed)

# Tree
# 决策树没有优化函数
```



### 训练分类器

`grid_search()`是类方法，会在优化时同步更新分类器的超参数，因此优化之后可以直接训练。

```python
# KNN
# KNN没有训练函数

# SVM
svm.ovr_trainer0()	# 基于ovr策略的分类器
svm.ovr_trainer1()	# 改进的基于ovr策略的分类器
svm,ovo_trainer0()	# 基于ovo策略的分类器

# Tree
tree.trainer0()
```

### 测试训练器

`test_function`也是类内方法，传入参数为文件路径和测试样本的占比，返回测试的错误率

```python
# KNN
knn.test_function(file_path, test_size)

# SVM
svm.test_function(file_path, test_size, 'ovo')
svm.test_function(file_path, test_size, 'ovr')

# Tree
tree.test_function(file_path, test_size)
```

## 一个例子

```python
import Classifier
import numpy as np

data_list, label_list = Classifier.load_file('./data/drug.txt')
data_arr = np.array(data_list)
num_data = data_arr.shape[0]

test_size = 0.13  # 取数据集的百分之13用作测试
num_test_iter = int(test_size * num_data)
data_list = data_list[num_test_iter: num_data]
label_list = label_list[num_test_iter:num_data]

"""
KNN
"""
# 创建KNN分类器对象，一部分数据集用作训练，一部分用作测试
knn = Classifier.KNN(data_list, label_list, 3)
params = {
    'k': range(3, 5)
}
print(knn.grid_search(params, 42))
# 测试分类器
knn_err_rate = knn.test_function('./data/drug.txt', test_size)
print(f"knn共测试了{num_test_iter}次，总的错误率是：{round(knn_err_rate * 100, 3)}%\n")

"""
SVM
"""
data_list = Classifier.standard_data(data_list)
# 创建SVM分类器对象
svm = Classifier.SVM(data_list, label_list, c=100, toler=1e-4, max_iter=90, k_tup=('rbf', 1))
params = {
    'model': 'ovr',  # 多分类器采用ovr还是ovo策略
    'kernel': ['rbf', 'line'],  # 核函数类型
    'c': range(1, 10, 100),  # 目标函数的惩罚系数
    'gamma': np.arange(1e-2, 1e-1, 1)  # 核函数的系数
}
print(svm.grid_search(params, seed=42))
svm.ovr_trainer1()
svm_err_rate2 = svm.test_function('./data/drug.txt', test_size, 'ovr')
print(f"svm共测试了{num_test_iter}次，总的错误率是：{round(svm_err_rate2 * 100, 3)}%\n")


"""
Tree
"""
test_size = 0.14  # 取数据集的百分之ho_ratio用作测试
num_test_iter = int(test_size * num_data)
# 创建决策树分类器对象
data_list, feature_list = Classifier.load_file('./data/drug.txt', 'Tree')
tree = Classifier.Tree(data_list[num_test_iter: num_data], feature_list)

tree_train_err_rate = tree.trainer0()
tree_test_err_rate = tree.test_function('./data/drug.txt', test_size)
print(f"tree训练错误率：{round(tree_train_err_rate * 100, 3)}%\n")
print(f"tree共测试了{num_test_iter}次，总的错误率是：{round(tree_test_err_rate, 5) * 100}%\n")


>>>
(3, 0.03846)
knn共测试了26次，总的错误率是：19.231%

(('line', 0.01, 1), 1.0)
svm共测试了26次，总的错误率是：38.462%

tree训练错误率：0.581%

tree共测试了28次，总的错误率是：0.0%
```

# sklearn库

下面是使用sklearn库对样本进行分类，做了简单的步骤合并

## 使用流程

### 数据处理

`file2data(file_path, test_size, random_state)`

用于读取csv文件中的数据，并转换成训练集、测试集

`file_path`是文件路径

`test_size`是测试集占比

`random_state`是随机数种子

下面是数据处理的主要步骤：

```python
# 加载数据集
df = pd.read_csv(file_path, engine="python")

# LabelEncoder对特征进行数字编码
label_encoder = LabelEncoder()
for cow in df:
    if type([cow][0]).__name__ == 'str':
        df[cow] = label_encoder.fit_transform(df[cow])

# 将数据分为特征信息（X）和标签（Y）
X = df.iloc[:, :-1]
Y = df.iloc[:, -1]

# train_test_split 函数进行数据集划分
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size,random_state=random_state)

# StandardScaler 将特征缩放到均值为0，标准差为1的范围内
std = StandardScaler()
x_train = std.fit_transform(x_train)
x_test = std.transform(x_test)

return x_train, x_test, y_train, y_test
```

### 创建模型

导入对应的模型

```python
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# 创建SVM模型
svm = SVC()

# KNN模型
knn = KNeighborsClassifier()
```

### 优化超参数

#### 网格搜索法

```python
# 创建SVM模型
svm = SVC(kernel='linear', C=1.0, gamma='auto')
# 定义参数网格
param_grid = {
    'kernel': ['linear', 'rbf'],
    'C': range(1, 10),
    'gamma': [1e-3, 1e-2, 1e-1, 1]
}
# 使用网格搜索
svm = GridSearchCV(svm, param_grid, cv=5)
```

#### 随机搜索

通过在给定的参数空间内**随机采样**来搜索最佳超参数组合。

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# 创建SVM模型
svm = SVC()
# 定义参数分布
param_dist = {
    'C': uniform(0, 10),
    'gamma': [1e-3, 1e-2, 1e-1, 1],
    'kernel': ['linear', 'rbf']
}
# 使用随机搜索
svm = RandomizedSearchCV(svm, param_dist, n_iter=20, cv=5)
```

#### 贝叶斯优化

利用贝叶斯推断来选择下一组超参数进行评估，从而高效地搜索参数空间。

```python
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer

# 创建SVM模型
svm = SVC()
# 定义参数范围
param_bayes = {
    'C': uniform(0, 10),
    'gamma': [1e-3, 1e-2, 1e-1, 1],
    'kernel': ['linear', 'rbf']
}
# 使用贝叶斯优化搜索
svm = BayesSearchCV(svm, param_bayes, n_iter=32, cv=5)
```

### 训练分类器

`fit`方法

```python
svm.fit(data_train, label_train)
```

### 测试分类器

`accuracy_function(model, x_test, y_test, report=False)`

用于导入测试数据，测试训练好的模型，返回只是测试错误率，可以选择打印分类报告

`model` 用于测试的模型

`x_test ` 测试集的特征数据

`y_test` 测试集的标签

`report` 是否打印分类报告，默认为 False

下面是主要步骤

```python
from sklearn.metrics import classification_report, accuracy_score
# 在测试集上进行预测
test_predictions = model.predict(x_test)

# 在测试集上评估模型
test_accuracy = accuracy_score(y_test, test_predictions)

# 打印分类报告
if report:
    print("\n分类报告:\n", classification_report(y_test, test_predictions))

return test_accuracy
```

## 一个例子

```python
if __name__ == '__main__':
    data_train, data_test, label_train, label_test = file2data('./drug200.csv', 0.3)

    # 创建SVM模型
    svm = SVC(kernel='linear', C=1.0, gamma='auto')
    # 定义参数网格
    param_grid = {
        'kernel': ['linear', 'rbf'],
        'C': range(1, 10),
        'gamma': [1e-3, 1e-2, 1e-1, 1]
    }
    # 使用网格搜索
    svm = GridSearchCV(svm, param_grid, cv=5)
    svm.fit(data_train, label_train)

    print("SVM 测试准确性:", round(accuracy_function(svm, data_test, label_test), 5) * 100, '%')

    # KNN模型
    knn = KNeighborsClassifier()
    knn.fit(data_train, label_train)
    # 定义参数网格
    param_grid = {
        'n_neighbors': range(1, 10)
    }
    # 使用网格搜索
    knn = GridSearchCV(knn, param_grid, cv=5)
    knn.fit(data_train, label_train)

    print("KNN 测试准确性:", round(accuracy_function(knn, data_test, label_test), 5) * 100, '%')
 
>>>
SVM 测试准确性: 98.333 %
KNN 测试准确性: 93.333 %
```

